{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7621ff99-34b0-4a66-91ae-43f13ec53253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import (create_shingles,\n",
    "                        calculate_accuracy,\n",
    "                        )\n",
    "\n",
    "from preprocess_data_script import preprocess_data_script\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "from pyspark.sql.functions import collect_list, collect_set, array\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import MinHashLSH, HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db23242-959d-4e1c-b16d-e3ebcd962c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = \"/mnt/c/Users/lniedzwiedzki/Desktop/PDD2024/Proteins_LSH_Project\"\n",
    "SHINGLE = 5\n",
    "BAND = 20\n",
    "ROW = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad444f5-ed84-4bc9-a89b-f1368b749e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PySpark 3.5.0 version is running...\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"LSH-Proteins\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "print(f'The PySpark {spark.version} version is running...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ab4fe3-9109-4cc3-b998-6e54d03bcc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25112, 2)\n",
      "(25112, 2)\n",
      "(25111, 3)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "df, protein_df, data = preprocess_data_script(spark, PROJECT_PATH)\n",
    "\n",
    "#df = spark.read.parquet(os.path.join(PROJECT_PATH, \"data\", \"df.parquet\"))\n",
    "#protein_df = spark.read.parquet(os.path.join(PROJECT_PATH, \"data\", \"proteins.parquet\"))\n",
    "#data = spark.read.parquet(os.path.join(PROJECT_PATH, \"data\", \"data.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c8152bb-a751-432e-bb52-6525282db6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|        cluster|      name|\n",
      "+---------------+----------+\n",
      "|UniRef50_Q8WZ42|    A2ASS6|\n",
      "|UniRef50_Q8WZ42|  Q8WZ42-8|\n",
      "|UniRef50_Q8WZ42|A0A2J8PRG4|\n",
      "|UniRef50_Q8WZ42|A0A2J8PRH0|\n",
      "|UniRef50_Q8WZ42|A0A2J8VRI6|\n",
      "|UniRef50_Q8WZ42|A0A2J8VRF7|\n",
      "|UniRef50_Q8WZ42|A0A8I5U7Y9|\n",
      "|UniRef50_Q8WZ42|  Q8WZ42-2|\n",
      "|UniRef50_Q8WZ42|A0A0C4DG59|\n",
      "|UniRef50_Q8WZ42|  Q8WZ42-7|\n",
      "|UniRef50_Q8WZ42|A0A2J8PRG6|\n",
      "|UniRef50_Q8WZ42|A0A2J8VRH1|\n",
      "|UniRef50_Q8WZ42|    C0JYZ2|\n",
      "|UniRef50_Q8WZ42| Q8WZ42-11|\n",
      "|UniRef50_Q8WZ42|    H2P803|\n",
      "|UniRef50_Q8WZ42|A0A6P8QXT8|\n",
      "|UniRef50_Q8WZ42|A0A6P8RJ11|\n",
      "|UniRef50_Q8WZ42|A0A8B7X843|\n",
      "|UniRef50_Q8WZ42|A0A091R2T7|\n",
      "|UniRef50_Q8WZ42|A0A7L3SFD7|\n",
      "+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9a5eaa-9ad7-426e-91f9-180e106cd257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      name|             protein|\n",
      "+----------+--------------------+\n",
      "|A0A014MIZ3|MSNDEKVLEYLKKLTAD...|\n",
      "|A0A022PR68|MKDSITKKAKSILKDEL...|\n",
      "|A0A026VSG9|MDLGGVRDITVRAGEDF...|\n",
      "|A0A034WAF9|FIIRRKWQKTGNAIRAL...|\n",
      "|A0A034WK66|VHEQVTPCEPNPCGSNA...|\n",
      "|A0A034WPU1|APPNCRPECVVSSECSQ...|\n",
      "|A0A034WTR2|VPNERAIFFVKIDCDED...|\n",
      "|A0A034WV39|AHLACMNTRCADPCVGS...|\n",
      "|A0A060W4G5|MWEAPEHDGGSPLTGYQ...|\n",
      "|A0A060W4N1|MSSSSMTEMMSHSHLEG...|\n",
      "|A0A060WEK7|MFIPESAVEIVRPPKDV...|\n",
      "|A0A060YYB5|MRLIWKLPANDGGERIK...|\n",
      "|A0A075R4X7|MNKDLIQLFSLTQPQQR...|\n",
      "|A0A075THI1|SSITVAWGKPIYDGGSE...|\n",
      "|A0A075TKX4|NSITVAWGKPIYDGGSE...|\n",
      "|A0A076EI29|MGEVPLTIAQYIELRGD...|\n",
      "|A0A087R0Q9|MVQHVHREFSPPSRLLR...|\n",
      "|A0A087R0R0|VSGRPAPVITWSKQGVD...|\n",
      "|A0A087R0R1|FRGKPVPNVTWNKSDTD...|\n",
      "|A0A087R0R2|IKGRPEPEVKWEKAEGT...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "protein_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b967773-1d7b-4c14-8853-918579caef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------------+\n",
      "|        cluster|      name|             protein|\n",
      "+---------------+----------+--------------------+\n",
      "|UniRef50_Q8WZ42|    A2ASS6|MTTQAPMFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|  Q8WZ42-8|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|A0A2J8PRG4|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|A0A2J8PRH0|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|A0A2J8VRI6|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|A0A2J8VRF7|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|A0A8I5U7Y9|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|  Q8WZ42-2|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|A0A0C4DG59|AAEAVATGAKEVKQDAD...|\n",
      "|UniRef50_Q8WZ42|  Q8WZ42-7|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|A0A2J8PRG6|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|A0A2J8VRH1|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|    C0JYZ2|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42| Q8WZ42-11|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|    H2P803|MTTQAPTFTQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|A0A6P8QXT8|MTTQAPTFLQPLQSVVT...|\n",
      "|UniRef50_Q8WZ42|A0A6P8RJ11|MTTQAPTFLQPLQSVVT...|\n",
      "|UniRef50_Q8WZ42|A0A8B7X843|MTTQAPTFKQPLQSVVV...|\n",
      "|UniRef50_Q8WZ42|A0A091R2T7|IKGRPTPEVTWTKDDVS...|\n",
      "|UniRef50_Q8WZ42|A0A7L3SFD7|RDVASAQWVAISSSSKK...|\n",
      "+---------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df99b5fa-c890-4f12-8489-4be99161add8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.alias('df')\n",
    "id(df) == id(data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "057c71db-ba55-41d1-816b-a09d0d4debce",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'functions'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Add shingles column to the DataFrame\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshingles\u001b[39m\u001b[38;5;124m\"\u001b[39m, shingle_udf(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotein\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create the HashingTF and MinHashLSH instances\u001b[39;00m\n\u001b[1;32m      7\u001b[0m hashingTF \u001b[38;5;241m=\u001b[39m HashingTF(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshingles\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, numFeatures\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1257\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'functions'\n"
     ]
    }
   ],
   "source": [
    "numHashTables = int(BAND * ROW)\n",
    "\n",
    "shingle_udf = udf(lambda x: create_shingles(x, k=SHINGLE), ArrayType(StringType()))\n",
    "\n",
    "df = df.withColumn(\"shingles\", shingle_udf(col(\"protein\")))\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"shingles\", outputCol=\"features\", numFeatures=1000)\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=numHashTables)\n",
    "\n",
    "# Transform the data to include the features\n",
    "featurizedData = hashingTF.transform(df)\n",
    "\n",
    "model = mh.fit(featurizedData)\n",
    "\n",
    "result = model.transform(featurizedData)\n",
    "\n",
    "lsh_clusters = model.approxSimilarityJoin(result, result, 1.0, distCol=\"distance\")\n",
    "\n",
    "# Select only the necessary columns and filter self-joins\n",
    "lsh_clusters = lsh_clusters.select(col(\"datasetA.cluster\").alias(\"clusterA\"),\n",
    "                                   col(\"datasetB.cluster\").alias(\"clusterB\"),\n",
    "                                   col(\"distance\")).filter(col(\"clusterA\") != col(\"clusterB\"))\n",
    "\n",
    "# Create a new DataFrame with unique cluster assignments\n",
    "cluster_assignments = lsh_clusters.groupBy(\"clusterA\").agg(collect_list(\"clusterB\").alias(\"lsh_cluster\"))\n",
    "\n",
    "df_with_lsh = df.join(cluster_assignments, df.cluster == cluster_assignments.clusterA, how=\"left\")\n",
    "\n",
    "# Select the final columns\n",
    "final_df = df_with_lsh.select(\"cluster\", \"name\", \"protein\", \"lsh_cluster\")\n",
    "\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40887678-0acd-4685-a146-087c2cc6c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, adjusted_mutual_info_score\n",
    "\n",
    "# Fill missing values in lsh_cluster with the original cluster values for comparison\n",
    "df_with_lsh = df_with_lsh.withColumn(\"lsh_cluster\", col(\"lsh_cluster\").cast(StringType()))\n",
    "df_with_lsh = df_with_lsh.withColumn(\"lsh_cluster\", udf(lambda x: x if x else [], ArrayType(StringType()))(col(\"lsh_cluster\")))\n",
    "df_with_lsh = df_with_lsh.withColumn(\"lsh_cluster\", udf(lambda x: x[0] if x else \"N/A\", StringType())(col(\"lsh_cluster\")))\n",
    "\n",
    "final_df = df_with_lsh.select(\"cluster\", \"lsh_cluster\")\n",
    "final_pd = final_df.toPandas()\n",
    "\n",
    "true_labels = final_pd['cluster']\n",
    "predicted_labels = final_pd['lsh_cluster']\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "fp = conf_matrix.sum(axis=0) - np.diag(conf_matrix)\n",
    "fn = conf_matrix.sum(axis=1) - np.diag(conf_matrix)\n",
    "adjusted_mi = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"False Positives: {fp.sum()}\")\n",
    "print(f\"False Negatives: {fn.sum()}\")\n",
    "print(f\"Adjusted Mutual Information Score: {adjusted_mi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a703bd2f-23e7-4a67-bf8f-e2338d9c6fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
